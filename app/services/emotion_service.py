# services/emotion_service.py
from typing import List, Dict, Optional
import os
from pathlib import Path
import torch
import numpy as np
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import json
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# KOTE 43 ë ˆì´ë¸” ì •ì˜
KOTE_LABELS = [
    'ë¶ˆí‰/ë¶ˆë§Œ', 'í™˜ì˜/í˜¸ì˜', 'ê°ë™/ê°íƒ„', 'ì§€ê¸‹ì§€ê¸‹', 'ê³ ë§ˆì›€', 'ìŠ¬í””', 'í™”ë‚¨/ë¶„ë…¸', 'ì¡´ê²½',
    'ê¸°ëŒ€ê°', 'ìš°ì­ëŒ/ë¬´ì‹œí•¨', 'ì•ˆíƒ€ê¹Œì›€/ì‹¤ë§', 'ë¹„ì¥í•¨', 'ì˜ì‹¬/ë¶ˆì‹ ', 'ë¿Œë“¯í•¨', 'í¸ì•ˆ/ì¾Œì ', 'ì‹ ê¸°í•¨/ê´€ì‹¬',
    'ì•„ê»´ì£¼ëŠ”', 'ë¶€ë„ëŸ¬ì›€', 'ê³µí¬/ë¬´ì„œì›€', 'ì ˆë§', 'í•œì‹¬í•¨', 'ì—­ê²¨ì›€/ì§•ê·¸ëŸ¬ì›€', 'ì§œì¦', 'ì–´ì´ì—†ìŒ', 'ì—†ìŒ',
    'íŒ¨ë°°/ìê¸°í˜ì˜¤', 'ê·€ì°®ìŒ', 'í˜ë“¦/ì§€ì¹¨', 'ì¦ê±°ì›€/ì‹ ë‚¨', 'ê¹¨ë‹¬ìŒ', 'ì£„ì±…ê°', 'ì¦ì˜¤/í˜ì˜¤',
    'íë­‡í•¨(ê·€ì—¬ì›€/ì˜ˆì¨)', 'ë‹¹í™©/ë‚œì²˜', 'ê²½ì•…', 'ë¶€ë‹´/ì•ˆ_ë‚´í‚´', 'ì„œëŸ¬ì›€', 'ì¬ë¯¸ì—†ìŒ',
    'ë¶ˆìŒí•¨/ì—°ë¯¼', 'ë†€ëŒ', 'í–‰ë³µ', 'ë¶ˆì•ˆ/ê±±ì •', 'ê¸°ì¨', 'ì•ˆì‹¬/ì‹ ë¢°'
]


class EmotionService:
    """
    ê°ì • ë¶„ì„ ì„œë¹„ìŠ¤ (KOTE 43 ë©€í‹°ë¼ë²¨ ê°ì • ë¶„ë¥˜ ëª¨ë¸)
    - ì‚¬ìš©ì ëŒ€í™” í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥ë°›ì•„ ìƒìœ„ 5ê°œ ê°ì •ì„ ë¶„ì„
    - kote-bert-ml ëª¨ë¸ ì‚¬ìš©
    """
    
    def __init__(self, model_path: Optional[str] = None):
        """
        Args:
            model_path: ëª¨ë¸ ë””ë ‰í† ë¦¬ ê²½ë¡œ (ì„ íƒ). ì—†ìœ¼ë©´ ê¸°ë³¸ ê²½ë¡œ ì‚¬ìš©
        """
        if model_path:
            self.model_dir = model_path
        else:
            # ê¸°ë³¸ ê²½ë¡œ: í”„ë¡œì íŠ¸ ë‚´ë¶€ ml_models/kote-bert-ml
            base_dir = Path(__file__).parent.parent.parent
            self.model_dir = str(base_dir / "ml_models" / "kote-bert-ml")
        
        self.device = None
        self.tokenizer = None
        self.model = None
        self.thresholds = None
        self._load_model()
        self._load_thresholds()
        
        # LLM ì´ˆê¸°í™” (ë¶€ì • ê°ì • í•„í„°ë§ìš©)
        self._init_llm()
    
    def _load_model(self):
        """
        ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
        """
        if not os.path.exists(self.model_dir):
            print(f"âš ï¸  ëª¨ë¸ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.model_dir}")
            self.model = None
            return
        
        try:
            print(f"ğŸ“¦ ê°ì • ë¶„ì„ ëª¨ë¸ ë¡œë”© ì¤‘... ({self.model_dir})")
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_dir)
            self.model = AutoModelForSequenceClassification.from_pretrained(self.model_dir)
            self.model.eval()  # í‰ê°€ ëª¨ë“œ
            
            # GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
            if torch.cuda.is_available():
                self.device = torch.device("cuda")
            elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                self.device = torch.device("mps")
            else:
                self.device = torch.device("cpu")
            
            self.model.to(self.device)
            print(f"âœ… ê°ì • ë¶„ì„ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (Device: {self.device})")
        except Exception as e:
            print(f"âš ï¸  ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.model = None
    
    def _load_thresholds(self):
        """
        Threshold íŒŒì¼ ë¡œë“œ
        """
        threshold_path = os.path.join(self.model_dir, "label_thresholds.json")
        if not os.path.exists(threshold_path):
            print(f"âš ï¸  Threshold íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {threshold_path}")
            self.thresholds = {}
            return
        
        try:
            with open(threshold_path, "r", encoding="utf-8") as f:
                self.thresholds = json.load(f)
            print("âœ… Threshold ë¡œë“œ ì™„ë£Œ")
        except Exception as e:
            print(f"âš ï¸  Threshold ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.thresholds = {}
    
    def _init_llm(self):
        """
        LLM ì´ˆê¸°í™” (ë¶€ì • ê°ì • í•„í„°ë§ìš©)
        """
        try:
            from app.config import get_settings
            settings = get_settings()
            
            self.llm = ChatOpenAI(
                model="gpt-4o-mini",
                api_key=settings.openai_api_key,
                temperature=0
            )
            
            # ë¶€ì • ê°ì • í•„í„°ë§ í”„ë¡¬í”„íŠ¸
            negative_filter_prompt_template = """ë‹¹ì‹ ì€ ê°ì • ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì£¼ì–´ì§„ ê°ì • ë¦¬ìŠ¤íŠ¸ì—ì„œ ë¶€ì •ì ì¸ ê°ì •ë§Œ í•„í„°ë§í•´ì£¼ì„¸ìš”.

[ê°ì • ë¦¬ìŠ¤íŠ¸]
{emotions_list}

[ê·œì¹™]
- ë¶€ì •ì ì¸ ê°ì •ë§Œ ì„ íƒí•˜ì„¸ìš” (ìŠ¬í””, ë¶„ë…¸, ë¶ˆì•ˆ, ì ˆë§, ì£„ì±…ê°, ì¦ì˜¤, ê³µí¬ ë“±)
- ê¸ì •ì ì¸ ê°ì •(ê¸°ì¨, í–‰ë³µ, ì¦ê±°ì›€, ê³ ë§ˆì›€ ë“±)ì€ ì œì™¸í•˜ì„¸ìš”
- ì¤‘ë¦½ì ì¸ ê°ì •(ì—†ìŒ, ë†€ëŒ ë“±)ì€ ì œì™¸í•˜ì„¸ìš”
- ê° ê°ì •ì˜ emotionê³¼ scoreë¥¼ ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ì„¸ìš”

[ì¶œë ¥ í˜•ì‹ (JSON ë°°ì—´)]
[
  {{"emotion": "ìŠ¬í””", "score": 0.9873, "threshold": 0.73, "is_active": true}},
  {{"emotion": "ë¶ˆì•ˆ/ê±±ì •", "score": 0.7234, "threshold": 0.58, "is_active": true}}
]

ë¶€ì •ì ì¸ ê°ì •ë§Œ JSON ë°°ì—´ë¡œ ë°˜í™˜í•˜ì„¸ìš”. ë‹¤ë¥¸ ì„¤ëª…ì€ í•˜ì§€ ë§ˆì„¸ìš”."""
            
            self.negative_filter_prompt = ChatPromptTemplate.from_template(negative_filter_prompt_template)
            self.output_parser = StrOutputParser()
            self.filter_chain = self.negative_filter_prompt | self.llm | self.output_parser
            
        except Exception as e:
            print(f"âš ï¸  LLM ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.llm = None
            self.filter_chain = None
    
    def analyze_emotions(self, text: str, top_k: int = 5) -> List[Dict[str, float]]:
        """
        í…ìŠ¤íŠ¸ì—ì„œ ìƒìœ„ Kê°œ ê°ì • ë¶„ì„
        
        Args:
            text: ë¶„ì„í•  í…ìŠ¤íŠ¸ (ëŒ€í™” ë‚´ìš©ì„ í•œ ì¤„ë¡œ í•©ì¹œ ê²ƒ)
            top_k: ë°˜í™˜í•  ê°ì • ê°œìˆ˜ (ê¸°ë³¸ 5ê°œ)
        
        Returns:
            ê°ì • ë¦¬ìŠ¤íŠ¸ [{"emotion": "ê¸°ì¨", "score": 0.85}, ...]
            ì ìˆ˜ ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
        """
        if not text or not text.strip():
            return []
        
        # ëª¨ë¸ì´ ìˆìœ¼ë©´ ëª¨ë¸ ì‚¬ìš©
        if self.model is not None:
            return self._predict_with_model(text, top_k)
        else:
            # ëª¨ë¸ì´ ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
            print("âš ï¸  ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•„ ê°ì • ë¶„ì„ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return []
    
    def _predict_with_model(self, text: str, top_k: int) -> List[Dict[str, float]]:
        """
        ì‹¤ì œ ëª¨ë¸ì„ ì‚¬ìš©í•œ ê°ì • ì˜ˆì¸¡
        """
        try:
            # í† í¬ë‚˜ì´ì§•
            inputs = self.tokenizer(
                text,
                return_tensors="pt",
                truncation=True,
                max_length=256,
                padding=True
            )
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            # ì¶”ë¡ 
            with torch.no_grad():
                outputs = self.model(**inputs)
                logits = outputs.logits[0]
                probs = torch.sigmoid(logits).cpu().numpy()
            
            # ìƒìœ„ Kê°œ ê°ì • ì¶”ì¶œ
            top_indices = np.argsort(-probs)[:top_k]
            results = []
            
            for idx in top_indices:
                emotion_label = KOTE_LABELS[int(idx)]
                score = float(probs[idx])
                threshold = self.thresholds.get(emotion_label, {}).get("thr", 0.5)
                
                results.append({
                    "emotion": emotion_label,
                    "score": round(score, 4),
                    "threshold": round(threshold, 4),
                    "is_active": score >= threshold
                })
            
            return results
            
        except Exception as e:
            print(f"âš ï¸  ê°ì • ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return []
    
    def combine_conversation_text(self, messages: List[Dict]) -> str:
        """
        ëŒ€í™” ë©”ì‹œì§€ë“¤ì„ í•œ ì¤„ í…ìŠ¤íŠ¸ë¡œ í•©ì¹˜ê¸°
        
        Args:
            messages: ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ [{"role": "user", "content": "...", ...}, ...]
        
        Returns:
            ì‚¬ìš©ì ë©”ì‹œì§€ë§Œ í•„í„°ë§í•˜ì—¬ ê³µë°±ìœ¼ë¡œ ì—°ê²°í•œ í…ìŠ¤íŠ¸
        """
        user_messages = []
        for msg in messages:
            if msg.get("role") == "user":
                content = msg.get("content", "").strip()
                if content:
                    user_messages.append(content)
        
        # ê³µë°±ìœ¼ë¡œ ì—°ê²°
        combined_text = " ".join(user_messages)
        return combined_text
    
    def filter_negative_emotions(self, emotions: List[Dict]) -> List[Dict]:
        """
        í”„ë¡¬í”„íŠ¸ë¥¼ ì´ìš©í•˜ì—¬ ë¶€ì •ì ì¸ ê°ì •ë§Œ í•„í„°ë§
        
        Args:
            emotions: ê°ì • ë¦¬ìŠ¤íŠ¸ [{"emotion": "...", "score": 0.85, ...}, ...]
        
        Returns:
            ë¶€ì •ì ì¸ ê°ì •ë§Œ í•„í„°ë§ëœ ë¦¬ìŠ¤íŠ¸
        """
        if not emotions:
            return []
        
        # LLMì´ ì—†ìœ¼ë©´ ì›ë³¸ ë°˜í™˜
        if self.filter_chain is None:
            print("âš ï¸  LLMì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•„ ë¶€ì • ê°ì • í•„í„°ë§ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            return emotions
        
        try:
            # ê°ì • ë¦¬ìŠ¤íŠ¸ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
            emotions_str = "\n".join([
                f"- {item['emotion']}: {item['score']:.4f} (threshold: {item['threshold']:.4f}, active: {item['is_active']})"
                for item in emotions
            ])
            
            # LLMìœ¼ë¡œ ë¶€ì • ê°ì • í•„í„°ë§
            result_str = self.filter_chain.invoke({"emotions_list": emotions_str})
            
            # JSON íŒŒì‹±
            # LLM ì‘ë‹µì—ì„œ JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ
            result_str = result_str.strip()
            
            # JSON ë°°ì—´ ë¶€ë¶„ë§Œ ì¶”ì¶œ (ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ì œê±°)
            if "```json" in result_str:
                result_str = result_str.split("```json")[1].split("```")[0].strip()
            elif "```" in result_str:
                result_str = result_str.split("```")[1].split("```")[0].strip()
            
            filtered_emotions = json.loads(result_str)
            
            # ì›ë³¸ emotionsì—ì„œ í•„í„°ë§ëœ ê²ƒë§Œ ì°¾ì•„ì„œ ë°˜í™˜ (ì›ë³¸ êµ¬ì¡° ìœ ì§€)
            filtered_emotion_names = {item["emotion"] for item in filtered_emotions}
            result = [
                emotion for emotion in emotions
                if emotion["emotion"] in filtered_emotion_names
            ]
            
            return result
            
        except json.JSONDecodeError as e:
            print(f"âš ï¸  JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
            print(f"   LLM ì‘ë‹µ: {result_str[:200]}...")
            # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì›ë³¸ ë°˜í™˜
            return emotions
        except Exception as e:
            print(f"âš ï¸  ë¶€ì • ê°ì • í•„í„°ë§ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì›ë³¸ ë°˜í™˜
            return emotions


# ì „ì—­ ê°ì • ë¶„ì„ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤
_emotion_service: Optional[EmotionService] = None

def get_emotion_service(model_path: Optional[str] = None) -> EmotionService:
    """
    ê°ì • ë¶„ì„ ì„œë¹„ìŠ¤ ì˜ì¡´ì„± ì£¼ì… (ì‹±ê¸€í†¤)
    """
    global _emotion_service
    if _emotion_service is None:
        _emotion_service = EmotionService(model_path=model_path)
    return _emotion_service

