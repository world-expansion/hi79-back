# utils/pdf_to_vector.py
"""
PDF íŒŒì¼ì„ PostgreSQL pgvectorì— ë²¡í„°ë¡œ ì €ì¥í•˜ëŠ” ìœ í‹¸ë¦¬í‹°
OpenAI text-embedding-3-small ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì„ë² ë”© ìƒì„± (í”„ë¡œì íŠ¸ í‘œì¤€)
"""

import os
import sys
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€ (ì–´ë””ì„œ ì‹¤í–‰í•˜ë“  ì‘ë™í•˜ë„ë¡)
_current_file = Path(__file__).resolve()
_project_root = _current_file.parent.parent.parent
if str(_project_root) not in sys.path:
    sys.path.insert(0, str(_project_root))

import fitz  # PyMuPDF
import psycopg2
import psycopg2.extras as extras
import time
from typing import List, Dict, Any, Tuple, Optional
from langchain_openai import OpenAIEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from app.config import get_settings
from openai import RateLimitError

# ê¸°ë³¸ ì„¤ì • - í”„ë¡œì íŠ¸ì™€ ë™ì¼í•œ ëª¨ë¸ ì‚¬ìš©
EMBED_MODEL_NAME = "text-embedding-3-small"  # 1536ì°¨ì› (í”„ë¡œì íŠ¸ í‘œì¤€)


def get_pdf_text_chunks(pdf_path: str) -> List[Dict[str, Any]]:
    """
    PDF íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ê³  ì²­í¬ë¡œ ë¶„í• 
    
    Args:
        pdf_path: PDF íŒŒì¼ ê²½ë¡œ
    
    Returns:
        ì²­í¬ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
    """
    if not os.path.exists(pdf_path):
        print(f"ì˜¤ë¥˜: '{pdf_path}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return []

    try:
        doc = fitz.open(pdf_path)
        full_text = ""
        for i in range(doc.page_count):
            full_text += doc.load_page(i).get_text() + "\n\n"
        doc.close()
    except Exception as e:
        print(f"ì˜¤ë¥˜: PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨. {e}")
        return []

    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1500,
        chunk_overlap=150,
        separators=["\n\n", "\n", " ", ""]
    )

    chunks = text_splitter.split_text(full_text)
    chunk_data = []
    pdf_file_id = os.path.basename(pdf_path)

    for i, chunk in enumerate(chunks):
        chunk_data.append({
            "id": f"{pdf_file_id}_chunk_{i}",
            "content": chunk,
            "metadata": {"source": pdf_file_id, "chunk_index": i}
        })

    print(f"ğŸ“„ PDFì—ì„œ ì´ {len(chunks)}ê°œì˜ í…ìŠ¤íŠ¸ ì²­í¬ë¥¼ ë¶„í• í–ˆìŠµë‹ˆë‹¤.")
    return chunk_data


def load_embedder(openai_api_key: Optional[str] = None) -> Tuple[OpenAIEmbeddings, int]:
    """
    ì„ë² ë”© ëª¨ë¸ ë¡œë“œ (í”„ë¡œì íŠ¸ í‘œì¤€ ëª¨ë¸ ì‚¬ìš©)
    
    Args:
        openai_api_key: OpenAI API í‚¤ (ì„ íƒ). ì—†ìœ¼ë©´ ì„¤ì •ì—ì„œ ê°€ì ¸ì˜´
    
    Returns:
        (ëª¨ë¸, ì°¨ì›) íŠœí”Œ
    """
    if not openai_api_key:
        try:
            settings = get_settings()
            openai_api_key = settings.openai_api_key
        except Exception:
            # ì„¤ì • ë¡œë“œ ì‹¤íŒ¨ ì‹œ í™˜ê²½ë³€ìˆ˜ì—ì„œ ì§ì ‘ ê°€ì ¸ì˜¤ê¸°
            openai_api_key = os.getenv("OPENAI_API_KEY")
    
    if not openai_api_key:
        raise ValueError("OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì— OPENAI_API_KEYë¥¼ ì„¤ì •í•˜ê±°ë‚˜ í™˜ê²½ë³€ìˆ˜ë¡œ ì„¤ì •í•˜ì„¸ìš”.")
    
    model = OpenAIEmbeddings(
        model=EMBED_MODEL_NAME,
        api_key=openai_api_key
    )
    dim = 1536  # text-embedding-3-smallì˜ ì°¨ì›
    print(f"ğŸ§  ì„ë² ë”© ëª¨ë¸ ë¡œë“œ: {EMBED_MODEL_NAME} (ì°¨ì›={dim})")
    return model, dim


def get_pg_conn(database_url: Optional[str] = None):
    """
    PostgreSQL ì—°ê²°
    
    Args:
        database_url: ë°ì´í„°ë² ì´ìŠ¤ URL (ì„ íƒ). ì—†ìœ¼ë©´ ì„¤ì • ë˜ëŠ” í™˜ê²½ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜´
    
    Returns:
        PostgreSQL ì—°ê²° ê°ì²´
    """
    if database_url:
        conn = psycopg2.connect(database_url)
    else:
        # ì„¤ì •ì—ì„œ ê°€ì ¸ì˜¤ê¸° ì‹œë„
        try:
            settings = get_settings()
            database_url = settings.database_url
        except Exception:
            # ì„¤ì • ë¡œë“œ ì‹¤íŒ¨ ì‹œ í™˜ê²½ë³€ìˆ˜ì—ì„œ ì§ì ‘ ê°€ì ¸ì˜¤ê¸°
            database_url = os.getenv("DATABASE_URL")
            if not database_url:
                # ê¸°ë³¸ê°’ ì‚¬ìš©
                database_url = os.getenv("PGDATABASE", "postgresql://postgres:password@localhost:5432/hi79_db")
        
        conn = psycopg2.connect(database_url)
    
    conn.autocommit = False
    return conn


def ensure_schema(conn, table_name: str, index_name: str, dim: int):
    """
    PostgreSQL ìŠ¤í‚¤ë§ˆ ì¤€ë¹„ (í…Œì´ë¸” ë° ì¸ë±ìŠ¤ ìƒì„±)
    
    Args:
        conn: PostgreSQL ì—°ê²°
        table_name: í…Œì´ë¸” ì´ë¦„
        index_name: ì¸ë±ìŠ¤ ì´ë¦„
        dim: ë²¡í„° ì°¨ì›
    """
    with conn.cursor() as cur:
        # í™•ì¥ì ë³´ì¥
        cur.execute("CREATE EXTENSION IF NOT EXISTS vector;")

        # í…Œì´ë¸” ìƒì„±
        cur.execute(f"""
        CREATE TABLE IF NOT EXISTS {table_name} (
            id TEXT PRIMARY KEY,
            content TEXT NOT NULL,
            source TEXT,
            chunk_index INT,
            embedding vector({dim})
        );
        """)

        # ë²¡í„° ì¸ë±ìŠ¤(ì½”ì‚¬ì¸) ìƒì„±
        cur.execute(f"""
        DO $$
        BEGIN
            IF NOT EXISTS (
                SELECT 1 FROM pg_class c
                JOIN pg_namespace n ON n.oid = c.relnamespace
                WHERE c.relname = '{index_name}' AND n.nspname = 'public'
            ) THEN
                EXECUTE 'CREATE INDEX {index_name}
                         ON {table_name}
                         USING ivfflat (embedding vector_cosine_ops)
                         WITH (lists = 100);';
            END IF;
        END$$;
        """)

    conn.commit()

    # ANALYZE (í†µê³„ ìˆ˜ì§‘)
    with conn.cursor() as cur:
        cur.execute(f"ANALYZE {table_name};")
    conn.commit()

    print(f"âœ… ìŠ¤í‚¤ë§ˆ ì¤€ë¹„ ì™„ë£Œ (í…Œì´ë¸”={table_name}, ì¸ë±ìŠ¤={index_name}, ì°¨ì›={dim})")


def upsert_chunks(conn, table_name: str, records: List[Dict[str, Any]], embeddings: List[List[float]]):
    """
    ì²­í¬ ë°ì´í„°ë¥¼ PostgreSQLì— ì—…ì„œíŠ¸
    
    Args:
        conn: PostgreSQL ì—°ê²°
        table_name: í…Œì´ë¸” ì´ë¦„
        records: ì²­í¬ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
        embeddings: ì„ë² ë”© ë¦¬ìŠ¤íŠ¸
    """
    rows = []
    for item, emb in zip(records, embeddings):
        vec_literal = "[" + ",".join(f"{x:.6f}" for x in emb) + "]"
        rows.append((
            item["id"],
            item["content"],
            item["metadata"]["source"],
            item["metadata"]["chunk_index"],
            vec_literal
        ))

    sql = f"""
    INSERT INTO {table_name} (id, content, source, chunk_index, embedding)
    VALUES %s
    ON CONFLICT (id) DO UPDATE
      SET content = EXCLUDED.content,
          source = EXCLUDED.source,
          chunk_index = EXCLUDED.chunk_index,
          embedding = EXCLUDED.embedding;
    """

    template = "(" + ",".join(["%s", "%s", "%s", "%s", "%s::vector"]) + ")"

    with conn.cursor() as cur:
        extras.execute_values(cur, sql, rows, template=template)

    conn.commit()
    print(f"ğŸ’¾ Postgresì— {len(rows)}ê°œ ì²­í¬ ì—…ì„œíŠ¸ ì™„ë£Œ.")


def search_similar(conn, table_name: str, query_text: str, model: OpenAIEmbeddings, top_k: int = 3):
    """
    ìœ ì‚¬í•œ ì²­í¬ ê²€ìƒ‰
    
    Args:
        conn: PostgreSQL ì—°ê²°
        table_name: í…Œì´ë¸” ì´ë¦„
        query_text: ê²€ìƒ‰ ì¿¼ë¦¬ í…ìŠ¤íŠ¸
        model: ì„ë² ë”© ëª¨ë¸
        top_k: ë°˜í™˜í•  ìƒìœ„ Kê°œ
    
    Returns:
        ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
    """
    q_emb = model.embed_query(query_text)
    vec_literal = "[" + ",".join(f"{x:.6f}" for x in q_emb) + "]"

    sql = f"""
    SELECT id, content, source, chunk_index
    FROM {table_name}
    ORDER BY embedding <=> %s::vector
    LIMIT %s;
    """

    with conn.cursor() as cur:
        cur.execute(sql, (vec_literal, top_k))
        rows = cur.fetchall()

    print("ğŸ” ê²€ìƒ‰ ê²°ê³¼ ì˜ˆì‹œ:")
    for rid, content, src, idx in rows:
        preview = content[:80].replace("\n", " ")
        print(f" - {rid} (src={src}, idx={idx}) | {preview}...")

    return rows


def process_pdf_to_vector(
    pdf_path: str,
    table_name: str = "thesis_chunks",
    index_name: str = "thesis_chunks_embedding_idx",
    database_url: Optional[str] = None
) -> bool:
    """
    PDF íŒŒì¼ì„ PostgreSQLì— ë²¡í„°ë¡œ ì €ì¥í•˜ëŠ” ë©”ì¸ í•¨ìˆ˜
    
    Args:
        pdf_path: PDF íŒŒì¼ ê²½ë¡œ
        table_name: ì €ì¥í•  í…Œì´ë¸” ì´ë¦„
        index_name: ë²¡í„° ì¸ë±ìŠ¤ ì´ë¦„
        database_url: ë°ì´í„°ë² ì´ìŠ¤ URL (ì„ íƒ)
    
    Returns:
        ì„±ê³µ ì—¬ë¶€
    """
    try:
        # 1) PDF â†’ ì²­í¬
        chunk_data = get_pdf_text_chunks(pdf_path)
        if not chunk_data:
            print("âŒ PDF ì²­í¬ ë¶„í•  ì‹¤íŒ¨")
            return False

        # 2) ì„ë² ë”
        embedder, dim = load_embedder()

        # 3) Postgres ìŠ¤í‚¤ë§ˆ ì¤€ë¹„
        conn = get_pg_conn(database_url)
        ensure_schema(conn, table_name, index_name, dim)

        # 4) ì„ë² ë”© ìƒì„± í›„ ì—…ì„œíŠ¸ (ì‘ì€ ë°°ì¹˜ë¡œ ì²˜ë¦¬)
        documents = [c["content"] for c in chunk_data]
        total_chunks = len(documents)
        batch_size = 10  # ì‘ì€ ë°°ì¹˜ í¬ê¸° (í• ë‹¹ëŸ‰ ê³ ë ¤)
        
        print(f"ğŸ“Š ì´ {total_chunks}ê°œ ì²­í¬ë¥¼ {batch_size}ê°œì”© ë°°ì¹˜ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤...")
        
        all_embeddings = []
        for i in range(0, total_chunks, batch_size):
            batch_docs = documents[i:i + batch_size]
            batch_num = i // batch_size + 1
            total_batches = (total_chunks - 1) // batch_size + 1
            
            print(f"ğŸ”„ ë°°ì¹˜ {batch_num}/{total_batches} ì²˜ë¦¬ ì¤‘... ({len(batch_docs)}ê°œ ì²­í¬)")
            
            try:
                batch_embeddings = embedder.embed_documents(batch_docs)
                all_embeddings.extend(batch_embeddings)
                print(f"âœ… ë°°ì¹˜ {batch_num} ì™„ë£Œ!")
            except RateLimitError as e:
                error_msg = str(e)
                # insufficient_quotaëŠ” ì¬ì‹œë„í•´ë„ í•´ê²°ë˜ì§€ ì•ŠìŒ
                if 'insufficient_quota' in error_msg.lower():
                    print(f"\nâŒ OpenAI API í• ë‹¹ëŸ‰ì´ ë¶€ì¡±í•©ë‹ˆë‹¤!")
                    print(f"   ê³„ì •ì˜ í• ë‹¹ëŸ‰ì„ í™•ì¸í•˜ê³  ê²°ì œ ì •ë³´ë¥¼ ì—…ë°ì´íŠ¸í•˜ì„¸ìš”.")
                    print(f"   https://platform.openai.com/account/billing")
                    print(f"   ì—ëŸ¬ ìƒì„¸: {error_msg}\n")
                    conn.close()
                    raise ValueError(
                        "OpenAI API í• ë‹¹ëŸ‰ì´ ë¶€ì¡±í•©ë‹ˆë‹¤. "
                        "ê³„ì •ì˜ í• ë‹¹ëŸ‰ì„ í™•ì¸í•˜ê³  ê²°ì œ ì •ë³´ë¥¼ ì—…ë°ì´íŠ¸í•œ í›„ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”."
                    )
                # ì¼ë°˜ rate limitì€ ì¬ì‹œë„ ê°€ëŠ¥í•˜ì§€ë§Œ ì—¬ê¸°ì„œëŠ” ì¦‰ì‹œ ì‹¤íŒ¨
                print(f"âŒ ë°°ì¹˜ {batch_num} ì˜¤ë¥˜: {e}")
                conn.close()
                raise
            except Exception as e:
                print(f"âŒ ë°°ì¹˜ {batch_num} ì˜¤ë¥˜: {e}")
                conn.close()
                raise
            
            # ë°°ì¹˜ ê°„ ì§§ì€ ëŒ€ê¸° (API ë¶€í•˜ ë¶„ì‚°)
            if i + batch_size < total_chunks:
                time.sleep(2)  # 2ì´ˆ ëŒ€ê¸°
        
        # ëª¨ë“  ì„ë² ë”©ì´ ì¤€ë¹„ë˜ë©´ í•œ ë²ˆì— ì—…ì„œíŠ¸
        print(f"ğŸ’¾ ì´ {len(all_embeddings)}ê°œ ì„ë² ë”©ì„ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ ì¤‘...")
        upsert_chunks(conn, table_name, chunk_data, all_embeddings)

        conn.close()
        print("âœ… PDF ë²¡í„° ì €ì¥ ì™„ë£Œ!")
        return True

    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        return False


# ë©”ì¸ ì‹¤í–‰ (ìŠ¤í¬ë¦½íŠ¸ë¡œ ì§ì ‘ ì‹¤í–‰í•  ë•Œ)
if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("ì‚¬ìš©ë²•:")
        print("  ë°©ë²• 1: python -m app.utils.pdf_to_vector <pdf_file_path> [table_name] [index_name]")
        print("  ë°©ë²• 2: python app/utils/pdf_to_vector.py <pdf_file_path> [table_name] [index_name]")
        print("\nì˜ˆì‹œ:")
        print("  python -m app.utils.pdf_to_vector data/000000228819_20251113005310.pdf")
        sys.exit(1)
    
    pdf_path = sys.argv[1]
    table_name = sys.argv[2] if len(sys.argv) > 2 else "thesis_chunks"
    index_name = sys.argv[3] if len(sys.argv) > 3 else "thesis_chunks_embedding_idx"
    
    # ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜ (í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê¸°ì¤€)
    if not os.path.isabs(pdf_path):
        # í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê¸°ì¤€ìœ¼ë¡œ ê²½ë¡œ ìƒì„±
        pdf_path = str(_project_root / pdf_path.lstrip('/'))
    
    success = process_pdf_to_vector(pdf_path, table_name, index_name)
    
    if success:
        print("\nâœ… ì²˜ë¦¬ ì™„ë£Œ!")
    else:
        print("\nâŒ ì²˜ë¦¬ ì‹¤íŒ¨!")
        sys.exit(1)

