# utils/txt_to_vector.py
"""
TXT íŒŒì¼ì„ PostgreSQL pgvectorì— ë²¡í„°ë¡œ ì €ì¥í•˜ëŠ” ìœ í‹¸ë¦¬í‹°
ì—¬ëŸ¬ txt íŒŒì¼ì„ ë°°ì¹˜ë¡œ ì²˜ë¦¬
"""

import os
import sys
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
_current_file = Path(__file__).resolve()
_project_root = _current_file.parent.parent.parent
if str(_project_root) not in sys.path:
    sys.path.insert(0, str(_project_root))

import psycopg2
import psycopg2.extras as extras
import time
from typing import List, Dict, Any, Optional
from langchain_openai import OpenAIEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from app.config import get_settings
from app.utils.pdf_to_vector import get_pg_conn, load_embedder, ensure_schema, upsert_chunks
from openai import RateLimitError

# ê¸°ë³¸ ì„¤ì •
EMBED_MODEL_NAME = "text-embedding-3-small"  # 1536ì°¨ì›


def get_txt_chunks(txt_path: str) -> List[Dict[str, Any]]:
    """
    TXT íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ê³  ì²­í¬ë¡œ ë¶„í• 
    
    Args:
        txt_path: TXT íŒŒì¼ ê²½ë¡œ
    
    Returns:
        ì²­í¬ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
    """
    if not os.path.exists(txt_path):
        print(f"âŒ ì˜¤ë¥˜: '{txt_path}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return []
    
    try:
        with open(txt_path, "r", encoding="utf-8") as f:
            full_text = f.read()
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜: TXT íŒŒì¼ ì½ê¸° ì‹¤íŒ¨. {e}")
        return []
    
    if not full_text.strip():
        print(f"âš ï¸  íŒŒì¼ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤: {txt_path}")
        return []
    
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1500,
        chunk_overlap=150,
        separators=["\n\n", "\n", " ", ""]
    )
    
    chunks = text_splitter.split_text(full_text)
    chunk_data = []
    txt_file_id = os.path.basename(txt_path)
    
    for i, chunk in enumerate(chunks):
        chunk_data.append({
            "id": f"{txt_file_id}_chunk_{i}",
            "content": chunk,
            "metadata": {"source": txt_file_id, "chunk_index": i}
        })
    
    print(f"ğŸ“„ {txt_file_id}ì—ì„œ ì´ {len(chunks)}ê°œì˜ í…ìŠ¤íŠ¸ ì²­í¬ë¥¼ ë¶„í• í–ˆìŠµë‹ˆë‹¤.")
    return chunk_data


def process_txt_to_vector(
    txt_path: str,
    table_name: str = "thesis_chunks",
    index_name: str = "thesis_chunks_embedding_idx",
    database_url: Optional[str] = None
) -> bool:
    """
    TXT íŒŒì¼ì„ PostgreSQLì— ë²¡í„°ë¡œ ì €ì¥í•˜ëŠ” ë©”ì¸ í•¨ìˆ˜
    
    Args:
        txt_path: TXT íŒŒì¼ ê²½ë¡œ
        table_name: ì €ì¥í•  í…Œì´ë¸” ì´ë¦„
        index_name: ë²¡í„° ì¸ë±ìŠ¤ ì´ë¦„
        database_url: ë°ì´í„°ë² ì´ìŠ¤ URL (ì„ íƒ)
    
    Returns:
        ì„±ê³µ ì—¬ë¶€
    """
    try:
        # 1) TXT â†’ ì²­í¬
        chunk_data = get_txt_chunks(txt_path)
        if not chunk_data:
            print(f"âŒ TXT ì²­í¬ ë¶„í•  ì‹¤íŒ¨: {txt_path}")
            return False
        
        # 2) ì„ë² ë”
        embedder, dim = load_embedder()
        
        # 3) Postgres ìŠ¤í‚¤ë§ˆ ì¤€ë¹„
        conn = get_pg_conn(database_url)
        ensure_schema(conn, table_name, index_name, dim)
        
        # 4) ì„ë² ë”© ìƒì„± í›„ ì—…ì„œíŠ¸ (ì‘ì€ ë°°ì¹˜ë¡œ ì²˜ë¦¬)
        documents = [c["content"] for c in chunk_data]
        total_chunks = len(documents)
        batch_size = 10  # ì‘ì€ ë°°ì¹˜ í¬ê¸° (í• ë‹¹ëŸ‰ ê³ ë ¤)
        
        print(f"ğŸ“Š ì´ {total_chunks}ê°œ ì²­í¬ë¥¼ {batch_size}ê°œì”© ë°°ì¹˜ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤...")
        
        all_embeddings = []
        for i in range(0, total_chunks, batch_size):
            batch_docs = documents[i:i + batch_size]
            batch_num = i // batch_size + 1
            total_batches = (total_chunks - 1) // batch_size + 1
            
            print(f"ğŸ”„ ë°°ì¹˜ {batch_num}/{total_batches} ì²˜ë¦¬ ì¤‘... ({len(batch_docs)}ê°œ ì²­í¬)")
            
            try:
                batch_embeddings = embedder.embed_documents(batch_docs)
                all_embeddings.extend(batch_embeddings)
                print(f"âœ… ë°°ì¹˜ {batch_num} ì™„ë£Œ!")
            except RateLimitError as e:
                error_msg = str(e)
                # insufficient_quotaëŠ” ì¬ì‹œë„í•´ë„ í•´ê²°ë˜ì§€ ì•ŠìŒ
                if 'insufficient_quota' in error_msg.lower():
                    print(f"\nâŒ OpenAI API í• ë‹¹ëŸ‰ì´ ë¶€ì¡±í•©ë‹ˆë‹¤!")
                    print(f"   ê³„ì •ì˜ í• ë‹¹ëŸ‰ì„ í™•ì¸í•˜ê³  ê²°ì œ ì •ë³´ë¥¼ ì—…ë°ì´íŠ¸í•˜ì„¸ìš”.")
                    print(f"   https://platform.openai.com/account/billing")
                    print(f"   ì—ëŸ¬ ìƒì„¸: {error_msg}\n")
                    conn.close()
                    raise ValueError(
                        "OpenAI API í• ë‹¹ëŸ‰ì´ ë¶€ì¡±í•©ë‹ˆë‹¤. "
                        "ê³„ì •ì˜ í• ë‹¹ëŸ‰ì„ í™•ì¸í•˜ê³  ê²°ì œ ì •ë³´ë¥¼ ì—…ë°ì´íŠ¸í•œ í›„ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”."
                    )
                # ì¼ë°˜ rate limitì€ ì¬ì‹œë„ ê°€ëŠ¥í•˜ì§€ë§Œ ì—¬ê¸°ì„œëŠ” ì¦‰ì‹œ ì‹¤íŒ¨
                print(f"âŒ ë°°ì¹˜ {batch_num} ì˜¤ë¥˜: {e}")
                conn.close()
                raise
            except Exception as e:
                print(f"âŒ ë°°ì¹˜ {batch_num} ì˜¤ë¥˜: {e}")
                conn.close()
                raise
            
            # ë°°ì¹˜ ê°„ ì§§ì€ ëŒ€ê¸° (API ë¶€í•˜ ë¶„ì‚°)
            if i + batch_size < total_chunks:
                time.sleep(2)  # 2ì´ˆ ëŒ€ê¸°
        
        # ëª¨ë“  ì„ë² ë”©ì´ ì¤€ë¹„ë˜ë©´ í•œ ë²ˆì— ì—…ì„œíŠ¸
        print(f"ğŸ’¾ ì´ {len(all_embeddings)}ê°œ ì„ë² ë”©ì„ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ ì¤‘...")
        upsert_chunks(conn, table_name, chunk_data, all_embeddings)
        
        conn.close()
        print(f"âœ… {os.path.basename(txt_path)} ë²¡í„° ì €ì¥ ì™„ë£Œ!")
        return True
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        return False


def process_all_txt_files(
    data_dir: str = "data",
    table_name: str = "thesis_chunks",
    database_url: Optional[str] = None
) -> Dict[str, bool]:
    """
    data í´ë”ì˜ ëª¨ë“  txt íŒŒì¼ì„ ë²¡í„°ë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥
    
    Args:
        data_dir: ë°ì´í„° í´ë” ê²½ë¡œ
        table_name: ì €ì¥í•  í…Œì´ë¸” ì´ë¦„
        database_url: ë°ì´í„°ë² ì´ìŠ¤ URL (ì„ íƒ)
    
    Returns:
        {íŒŒì¼ëª…: ì„±ê³µì—¬ë¶€} ë”•ì…”ë„ˆë¦¬
    """
    data_path = Path(_project_root) / data_dir
    if not data_path.exists():
        print(f"âŒ ì˜¤ë¥˜: '{data_dir}' í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return {}
    
    # ëª¨ë“  txt íŒŒì¼ ì°¾ê¸°
    txt_files = sorted([f for f in data_path.glob("*.txt")])
    
    if not txt_files:
        print(f"âŒ '{data_dir}' í´ë”ì— txt íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return {}
    
    print(f"ğŸ“ ì´ {len(txt_files)}ê°œì˜ txt íŒŒì¼ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.\n")
    
    results = {}
    index_name = f"{table_name}_embedding_idx"
    
    for i, txt_file in enumerate(txt_files, 1):
        print(f"\n{'='*60}")
        print(f"[{i}/{len(txt_files)}] ì²˜ë¦¬ ì¤‘: {txt_file.name}")
        print(f"{'='*60}\n")
        
        success = process_txt_to_vector(
            str(txt_file),
            table_name=table_name,
            index_name=index_name,
            database_url=database_url
        )
        
        results[txt_file.name] = success
        
        # íŒŒì¼ ê°„ ëŒ€ê¸° (API ë¶€í•˜ ë¶„ì‚°)
        if i < len(txt_files):
            print(f"\nâ³ ë‹¤ìŒ íŒŒì¼ ì²˜ë¦¬ ì „ 3ì´ˆ ëŒ€ê¸°...\n")
            time.sleep(3)
    
    # ê²°ê³¼ ìš”ì•½
    print(f"\n{'='*60}")
    print("ğŸ“Š ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½")
    print(f"{'='*60}")
    success_count = sum(1 for v in results.values() if v)
    fail_count = len(results) - success_count
    
    for filename, success in results.items():
        status = "âœ… ì„±ê³µ" if success else "âŒ ì‹¤íŒ¨"
        print(f"  {status}: {filename}")
    
    print(f"\nì´ {len(results)}ê°œ íŒŒì¼ ì¤‘ {success_count}ê°œ ì„±ê³µ, {fail_count}ê°œ ì‹¤íŒ¨")
    print(f"{'='*60}\n")
    
    return results


# ë©”ì¸ ì‹¤í–‰
if __name__ == "__main__":
    if len(sys.argv) > 1 and sys.argv[1] == "--all":
        # ëª¨ë“  txt íŒŒì¼ ì²˜ë¦¬
        data_dir = sys.argv[2] if len(sys.argv) > 2 else "data"
        results = process_all_txt_files(data_dir)
        
        if all(results.values()):
            print("\nâœ… ëª¨ë“  íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ!")
            sys.exit(0)
        else:
            print("\nâš ï¸  ì¼ë¶€ íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨")
            sys.exit(1)
    elif len(sys.argv) > 1:
        # ë‹¨ì¼ íŒŒì¼ ì²˜ë¦¬
        txt_path = sys.argv[1]
        table_name = sys.argv[2] if len(sys.argv) > 2 else "thesis_chunks"
        index_name = sys.argv[3] if len(sys.argv) > 3 else "thesis_chunks_embedding_idx"
        
        # ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
        if not os.path.isabs(txt_path):
            txt_path = str(_project_root / txt_path.lstrip('/'))
        
        success = process_txt_to_vector(txt_path, table_name, index_name)
        
        if success:
            print("\nâœ… ì²˜ë¦¬ ì™„ë£Œ!")
            sys.exit(0)
        else:
            print("\nâŒ ì²˜ë¦¬ ì‹¤íŒ¨!")
            sys.exit(1)
    else:
        print("ì‚¬ìš©ë²•:")
        print("  ë‹¨ì¼ íŒŒì¼: python app/utils/txt_to_vector.py <txt_file_path>")
        print("  ëª¨ë“  íŒŒì¼: python app/utils/txt_to_vector.py --all [data_dir]")
        print("\nì˜ˆì‹œ:")
        print("  python app/utils/txt_to_vector.py --all data")
        print("  python app/utils/txt_to_vector.py data/ìˆ˜ë°˜ì„± ê´€ë¦¬.txt")
        sys.exit(1)

